# Repositorio de Prueba TÃ©cnica Sudata

Este repositorio contiene la resoluciÃ³n de distintos ejercicios como parte de una prueba tÃ©cnica. Cada ejercicio se encuentra organizado en su propia carpeta con cÃ³digo fuente, scripts, y un `README.md` explicativo.

---

## ğŸ“ Estructura general

```text
Ejercicio 1/
â”œâ”€â”€ replicate.py        # Script Python de replicaciÃ³n
â”œâ”€â”€ create_db.py        # Script Python para crear la base de datos y las tablas en PostgreSQL
â”œâ”€â”€ csv_to_db.py        # Script Python para poblar las tablas con datos
â”œâ”€â”€ .env.example        # Ejemplo de archivo con variables de entorno
â”œâ”€â”€ README.md           # DocumentaciÃ³n principal para este ejercicio
â””â”€â”€ screenshots/        # Capturas de pantalla del Programador de Tareas en Windows
    â”œâ”€â”€ Esqueda DB      # Screenshot del esquema creado en Supabase
    â””â”€â”€ pasos.png       # Imagenes explicativas

Ejercicio 2/
â”œâ”€â”€ data_historica.py   # Script principal para extraer las cotizaciones histÃ³ricas desde la API del BCRA
â”œâ”€â”€ incremental.py      # Script para ingresar los nuevos datos a partir de la ultima fecha registrada en la base de datos
â”œâ”€â”€ utils.py            # Todas las funciones necesarias para mantener el cÃ³digo limpio y escalable
â”œâ”€â”€ .env.example        # Ejemplo con variables necesarias
â””â”€â”€ README.md           # DocumentaciÃ³n detallada del Ejercicio 2

<<<<<<< HEAD
Ejercicio 3/
â”œâ”€â”€ .env                 # variables de entorno (no subir)
â”œâ”€â”€ bloqueos_tecnicos.py # detectar Cloudflare / CAPTCHA / rate-limits
â”œâ”€â”€ csv_to_db_supabase.py# leer CSV, limpiar/transformar y cargar a Supabase
â”œâ”€â”€ permite_scrap.py     # comprobar robots.txt / permiso de crawling
â”œâ”€â”€ scraping.py          # scraper para Argenprop (genera CSV)
â””â”€â”€ README.md            # DocumentaciÃ³n especÃ­fica del Ejercicio 3

=======
>>>>>>> 9a8f206c1285824160e2575ba4d106a51139baae
â””â”€â”€ README.md           # Este README global
````

---

## ğŸ“Œ Contenidos por ejercicio

### ğŸ”¹ Ejercicio 1: ReplicaciÃ³n PostgreSQL â†’ Supabase

* Crear una base PostgreSQL local (`ventas_origen`)
* Crear y poblar tablas con datos desde CSV
* Replicar los datos a una base espejo en Supabase (`ventas_espejo`)
* Automatizar la replicaciÃ³n diaria con Task Scheduler (Programador de Tareas de Windows)

Ver [Ejercicio 1/README.md](Ejercicio%201/README.md) para mÃ¡s detalles.

---

### ğŸ”¹ Ejercicio 2: ExtracciÃ³n incremental desde la API del BCRA usando Render cron jobs

* Se conecta a la [API oficial del BCRA](https://www.bcra.gob.ar/BCRAyVos/catalogo-de-APIs-banco-central.asp) para obtener las cotizaciones del dÃ³lar tipo vendedor.
* La extracciÃ³n es **incremental**, es decir, se descarga solo la informaciÃ³n nueva desde la Ãºltima fecha registrada.
* Se utiliza una base PostgreSQL en la nube desplegada en **Render**.
* La actualizaciÃ³n semanal se realiza mediante un **cron job configurado en Render** (sin necesidad de Task Scheduler ni GitHub Actions).

Ver [Ejercicio 2/README.md](Ejercicio%202/README.md) para mÃ¡s detalles.

---

### ğŸ”¹ Ejercicio 3: Scraping responsable â†’ Limpieza â†’ Carga a Supabase

**Resumen:** herramientas y scripts para comprobar permisos y bloqueos tÃ©cnicos, raspar listados pÃºblicos (ej. Argenprop â€” `terrenos/venta/posadas`), limpiar/transformar el CSV resultante y cargarlo a una base Postgres alojada en **Supabase**.

**QuÃ© contiene la carpeta `Ejercicio 3/`:**

* **`.env`** â€” variables de entorno (no subir al repo). Variables esperadas:
  `SUPABASE_DB_USER`, `SUPABASE_DB_PASSWORD`, `SUPABASE_DB_HOST`, `SUPABASE_DB_PORT`, `SUPABASE_DB_NAME`.

* **`permite_scrap.py`** â€” comprueba `robots.txt` y ayuda a interpretar si la ruta objetivo estÃ¡ permitida para crawling con un user-agent dado. Es la primera verificaciÃ³n recomendada antes de raspar.

* **`bloqueos_tecnicos.py`** â€” pruebas tÃ©cnicas que detectan bloqueos: cÃ³digos HTTP (403/429/503), cabeceras Cloudflare / `cf-`, texto de desafÃ­o (CAPTCHA), etc. Sirve para saber si el sitio aplica protecciÃ³n anti-bot.

* **`scraping.py`** â€” scraper principal (Selenium + utilidades). Funciones principales:

  * `init_driver(headless=True)`: inicializa Chrome con opciones (headless opcional, user-agent, evasiÃ³n bÃ¡sica de `navigator.webdriver`).
  * `close_cookies_if_present(driver)`, `scroll_page(driver)`, `extract_cards_on_page(driver)`, `click_next_page(driver)` â€” helpers para interactuar con la pÃ¡gina y extraer tarjetas.
  * Genera un CSV con columnas crudas: `precio_raw`, `moneda`, `ubicacion`, `titulo_primary`, `detalle_url`, etc.

* **`csv_to_db_supabase.py`** â€” lee el CSV, aplica transformaciones (normaliza nombres de columna, filtra filas sin `moneda`, convierte `precio` a `int`, agrega `id` secuencial) y carga a Supabase usando **SQLAlchemy**. Crea la tabla si no existe y usa `INSERT ... ON CONFLICT DO NOTHING` por defecto.

**Flujo recomendado (resumido):**

1. Revisar TÃ©rminos y Condiciones y ejecutar `permite_scrap.py`.
2. Ejecutar `bloqueos_tecnicos.py --url "<tu_url_objetivo>"`.
3. Si estÃ¡ permitido y no hay bloqueos severos, ejecutar `scraping.py` â†’ generar CSV.
4. Ajustar `.env` con credenciales Supabase y ejecutar `csv_to_db_supabase.py` â†’ datos en Supabase.

**Nota legal / buenas prÃ¡cticas:** incluso si los avisos son visibles pÃºblicamente, los TÃ©rminos pueden prohibir la extracciÃ³n automatizada. Pedir permiso por escrito (ej. `info@argenprop.com`) antes de raspar a gran escala o uso comercial. Respetar `robots.txt`, `crawl-delay` y no evadir CAPTCHAs.

Ver [Ejercicio 3/README.md](Ejercicio%203/README.md) para la documentaciÃ³n completa del ejercicio 3 (comandos, ejemplos y troubleshooting).

---

## âœ… Recomendaciones de uso

* Ingresar a cada carpeta de ejercicio para acceder a sus scripts y documentaciÃ³n especÃ­fica.
* Usar entornos virtuales para instalar dependencias locales.
* Documentar cada nuevo ejercicio agregando su propia carpeta y README.
* Mantener el archivo `.env` fuera del control de versiones (aÃ±adirlo a `.gitignore`).

---

## ğŸ“„ Licencia

Este repositorio estÃ¡ disponible bajo la licencia MIT.